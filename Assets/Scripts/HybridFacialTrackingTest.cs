using UnityEngine;
using UnityEngine.XR.OpenXR;
using VIVE.OpenXR.FacialTracking;
using VIVE.OpenXR.Samples.FacialTracking;

/// <summary>
/// í•˜ì´ë¸Œë¦¬ë“œ ì–¼êµ´ íŠ¸ë˜í‚¹ í…ŒìŠ¤íŠ¸ - Mock Runtimeê³¼ ì‹¤ì œ ê¸°ê¸° ëª¨ë‘ ì§€ì›
/// </summary>
public class HybridFacialTrackingTest : MonoBehaviour
{
    [Header("Mode")]
    public bool useMockData = false;
    public bool autoDetectMockRuntime = true;
    
    [Header("Mock Controls (Q/W/A/S/Z/X/C/V)")]
    public MockFacialTrackingTest mockController;
    
    [Header("Real Device")]
    private ViveFacialTracking facialTrackingFeature;
    private float[] eyeBlendShapes;
    private float[] lipBlendShapes;
    private bool isInitialized = false;
    
    void Start()
    {
        // Mock Runtime ìë™ ê°ì§€
        if (autoDetectMockRuntime)
        {
            var mockRuntimeFeature = OpenXRSettings.Instance?.GetFeature<VIVE.OpenXR.Feature.ViveMockRuntime>();
            if (mockRuntimeFeature != null && mockRuntimeFeature.enabled)
            {
                useMockData = true;
                Debug.LogWarning("ğŸ® Mock Runtime detected - switching to keyboard simulation mode");
            }
        }
        
        if (useMockData)
        {
            InitializeMockMode();
        }
        else
        {
            Invoke("InitializeRealDevice", 3f);
        }
    }
    
    void InitializeMockMode()
    {
        // Mock ì»¨íŠ¸ë¡¤ëŸ¬ê°€ ì—†ìœ¼ë©´ ìë™ ìƒì„±
        if (mockController == null)
        {
            mockController = gameObject.AddComponent<MockFacialTrackingTest>();
        }
        
        Debug.Log("ğŸ® Mock Mode Active - Use keyboard:");
        Debug.Log("   Q/W: Eye Blink, A/S: Eye Wide");
        Debug.Log("   Z: Jaw Open, X: Pout, C/V: Smile");
        isInitialized = true;
    }
    
    void InitializeRealDevice()
    {
        facialTrackingFeature = OpenXRSettings.Instance?.GetFeature<ViveFacialTracking>();
        
        if (facialTrackingFeature == null || !facialTrackingFeature.enabled)
        {
            Debug.LogError("âŒ ViveFacialTracking not available - switching to mock mode");
            useMockData = true;
            InitializeMockMode();
            return;
        }
        
        // íŠ¸ë˜ì»¤ ìƒì„± ì‹œë„
        bool hasSupport = false;
        
        if (facialTrackingFeature.CreateFacialTracker(XrFacialTrackingTypeHTC.XR_FACIAL_TRACKING_TYPE_EYE_DEFAULT_HTC))
        {
            Debug.Log("âœ… Eye tracker created");
            hasSupport = true;
        }
        
        if (facialTrackingFeature.CreateFacialTracker(XrFacialTrackingTypeHTC.XR_FACIAL_TRACKING_TYPE_LIP_DEFAULT_HTC))
        {
            Debug.Log("âœ… Lip tracker created");
            hasSupport = true;
        }
        
        if (!hasSupport)
        {
            Debug.LogWarning("âš ï¸ No facial tracking support - switching to mock mode");
            useMockData = true;
            InitializeMockMode();
            return;
        }
        
        isInitialized = true;
        Debug.Log("âœ… Real device facial tracking ready!");
    }
    
    void Update()
    {
        if (!isInitialized) return;
        
        if (useMockData)
        {
            // Mock ë°ì´í„° ì‚¬ìš©
            ProcessMockData();
        }
        else
        {
            // ì‹¤ì œ ê¸°ê¸° ë°ì´í„° ì‚¬ìš©
            ProcessRealData();
        }
    }
    
    void ProcessMockData()
    {
        if (mockController == null) return;
        
        // Mock ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        var mockEyeData = mockController.GetMockEyeBlendShapes();
        var mockLipData = mockController.GetMockLipBlendShapes();
        
        // ì—¬ê¸°ì„œ mockEyeDataì™€ mockLipDataë¥¼ ì‚¬ìš©í•´ì„œ ì•„ë°”íƒ€ ì—…ë°ì´íŠ¸ ë“± ê°€ëŠ¥
        // ì˜ˆ: UpdateAvatar(mockEyeData, mockLipData);
    }
    
    void ProcessRealData()
    {
        if (facialTrackingFeature == null) return;
        
        // Eye Tracking
        if (facialTrackingFeature.GetFacialExpressions(XrFacialTrackingTypeHTC.XR_FACIAL_TRACKING_TYPE_EYE_DEFAULT_HTC, out eyeBlendShapes))
        {
            // ì‹¤ì œ eye ë°ì´í„° ì²˜ë¦¬
            DetectEyeExpressions(eyeBlendShapes);
        }
        
        // Lip Tracking
        if (facialTrackingFeature.GetFacialExpressions(XrFacialTrackingTypeHTC.XR_FACIAL_TRACKING_TYPE_LIP_DEFAULT_HTC, out lipBlendShapes))
        {
            // ì‹¤ì œ lip ë°ì´í„° ì²˜ë¦¬
            DetectLipExpressions(lipBlendShapes);
        }
    }
    
    void DetectEyeExpressions(float[] shapes)
    {
        float leftBlink = shapes[(int)XrEyeShapeHTC.XR_EYE_EXPRESSION_LEFT_BLINK_HTC];
        float rightBlink = shapes[(int)XrEyeShapeHTC.XR_EYE_EXPRESSION_RIGHT_BLINK_HTC];
        
        if (leftBlink > 0.1f || rightBlink > 0.1f)
            Debug.Log($"ğŸ‘ï¸ Blink - L: {leftBlink:F2}, R: {rightBlink:F2}");
    }
    
    void DetectLipExpressions(float[] shapes)
    {
        float jawOpen = shapes[(int)XrLipShapeHTC.XR_LIP_SHAPE_JAW_OPEN_HTC];
        
        if (jawOpen > 0.1f)
            Debug.Log($"ğŸ‘„ Jaw Open: {jawOpen:F2}");
    }
    
    // ì™¸ë¶€ì—ì„œ í˜„ì¬ ë¸”ë Œë“œì…°ì´í”„ ê°’ ê°€ì ¸ì˜¤ê¸°
    public float[] GetCurrentEyeBlendShapes()
    {
        if (useMockData && mockController != null)
            return mockController.GetMockEyeBlendShapes();
        else
            return eyeBlendShapes ?? new float[(int)XrEyeShapeHTC.XR_EYE_EXPRESSION_MAX_ENUM_HTC];
    }
    
    public float[] GetCurrentLipBlendShapes()
    {
        if (useMockData && mockController != null)
            return mockController.GetMockLipBlendShapes();
        else
            return lipBlendShapes ?? new float[(int)XrLipShapeHTC.XR_LIP_SHAPE_MAX_ENUM_HTC];
    }
    
    void OnDestroy()
    {
        if (!useMockData && facialTrackingFeature != null && isInitialized)
        {
            facialTrackingFeature.DestroyFacialTracker(XrFacialTrackingTypeHTC.XR_FACIAL_TRACKING_TYPE_EYE_DEFAULT_HTC);
            facialTrackingFeature.DestroyFacialTracker(XrFacialTrackingTypeHTC.XR_FACIAL_TRACKING_TYPE_LIP_DEFAULT_HTC);
        }
    }
} 